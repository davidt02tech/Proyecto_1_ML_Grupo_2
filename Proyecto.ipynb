{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from  skimage.io import imread, imshow\n",
    "from knn import KNN\n",
    "from svm import SVM\n",
    "from reglog import Regresion\n",
    "import pywt\n",
    "import pywt.data\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "def redimensionar(path):\n",
    "   imagenes=[]\n",
    "   entries = Path(path)\n",
    "   for entry in entries.iterdir():\n",
    "      imagenes.append(path + entry.name)\n",
    "\n",
    "   # Tamaño deseado para todas las imágenes\n",
    "   ancho_deseado = 300\n",
    "   alto_deseado = 200\n",
    "\n",
    "   # Carpeta de destino para las imágenes redimensionadas\n",
    "   carpeta_destino = \"imagenes_1\"\n",
    "\n",
    "   # Crea la carpeta de destino si no existe\n",
    "   if not os.path.exists(carpeta_destino):\n",
    "      os.makedirs(carpeta_destino)\n",
    "\n",
    "   # Recorre la lista de imágenes y redimensiona y guarda cada una\n",
    "   for ruta_imagen in imagenes:\n",
    "      imagen = Image.open(ruta_imagen)\n",
    "      \n",
    "      # Redimensiona la imagen al tamaño deseado con interpolación ANTIALIAS\n",
    "      imagen_redimensionada = imagen.resize((ancho_deseado, alto_deseado), Image.ADAPTIVE)\n",
    "      \n",
    "      # Obtiene el nombre de archivo sin la ruta\n",
    "      nombre_archivo = os.path.basename(ruta_imagen)\n",
    "      \n",
    "      # Define la ruta de archivo de destino en la carpeta de destino\n",
    "      ruta_destino = os.path.join(carpeta_destino, nombre_archivo)\n",
    "      \n",
    "      # Guarda la imagen redimensionada en la carpeta de destino\n",
    "      imagen_redimensionada.save(ruta_destino)     \n",
    "\n",
    "def Get_Feacture(picture, cortes):\n",
    "  LL = picture\n",
    "  for i in range(cortes):\n",
    "     LL, (LH, HL, HH) = pywt.dwt2(LL, 'haar')\n",
    "  return LL.flatten().tolist()\n",
    "\n",
    "def cargar_dataset():\n",
    "\n",
    "    carpeta_imagenes = './imagenes_1/'\n",
    "\n",
    "    # Se ordena los archivos segun el nombre asignado\n",
    "    archivos = os.listdir(carpeta_imagenes)\n",
    "\n",
    "    clases = []\n",
    "    vectores_caracteristicos = []\n",
    "\n",
    "\n",
    "    for archivo in archivos:\n",
    "        # Verifica si el archivo es una imagen (puedes agregar más extensiones si es necesario)\n",
    "        if archivo.endswith(('.png')):\n",
    "            # Construye la ruta completa del archivo\n",
    "            ruta_completa = os.path.join(carpeta_imagenes, archivo)\n",
    "            imagen = imread(ruta_completa)\n",
    "\n",
    "            # Se añade la clase correspondiente al vector clases \n",
    "            if int(archivo[1]) < 1:\n",
    "                clases.append(int(archivo[2]))\n",
    "            else:\n",
    "                clases.append(int(archivo[1:3]))\n",
    "\n",
    "            # Se añade el vector caracteristico de cada imagen\n",
    "            vectores_caracteristicos.append(Get_Feacture(imagen, 2))\n",
    "\n",
    "    return vectores_caracteristicos,clases\n",
    "\n",
    "def acuary(y_prueba,y_correct):\n",
    "    correctos= np.sum(y_prueba == y_correct)\n",
    "    return (correctos/len(y_correct))*100      \n",
    "\n",
    "  \n",
    "# Redimensionamos las imagenes para trabajar todas con el mismo tamaño\n",
    "redimensionar(\"./images/\")\n",
    "\n",
    "# Definimos el dataset\n",
    "x,y= cargar_dataset()\n",
    "\n",
    "x=np.array(x)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Regresión Logistica\n",
    "\n",
    "#### Validacion por division del dataset en 70% entrenamiento 15% validación, 15% prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los tamaños de los conjuntos (70% entrenamiento, 15% validación, 15% prueba)\n",
    "total_samples = len(x)\n",
    "train = int(0.7 * total_samples)\n",
    "validation = int(0.15 * total_samples)\n",
    "\n",
    "# Dividimoss los datos en conjuntos de entrenamiento, validación y prueba\n",
    "X_train = x[:train]\n",
    "y_train = y[:train]\n",
    "\n",
    "X_val = x[train:train + validation]\n",
    "y_val = y[train:train + validation]\n",
    "\n",
    "X_test = x[train + validation:]\n",
    "y_test = y[train + validation:]\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "modelo = Regresion(1000,0.00000001)\n",
    "modelo.train(X_train, y_train)\n",
    "\n",
    "# Validacion\n",
    "y_pred_val = modelo.predict(X_val)\n",
    "precision_val=acuary(y_val,y_pred_val)\n",
    "\n",
    "print(f\"Precisión en el conjunto de validación: {precision_val:.2f}\")\n",
    "\n",
    "# Predicción\n",
    "y_pred_test = modelo.predict(X_test)\n",
    "precision_test=acuary(y_test,y_pred_test)\n",
    "\n",
    "print(f\"Precisión en el conjunto de testeo: {precision_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validacion por K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el número de folds\n",
    "k = 5\n",
    "\n",
    "# Mezclamos los datos aleatoriamente\n",
    "indices = np.arange(len(x))\n",
    "np.random.shuffle(indices)\n",
    "X = x[indices]\n",
    "Y = y[indices]\n",
    "\n",
    "\n",
    "# Divide los datos en k grupos\n",
    "X_folds = np.array_split(X, k)\n",
    "Y_folds = np.array_split(Y, k)\n",
    "\n",
    "\n",
    "\n",
    "# Lista para almacenar los porcentajes de rendimiento\n",
    "accuracies=[]\n",
    "\n",
    "# Realizamos k iteraciones\n",
    "for i in range(k):\n",
    "    # Selecciona el conjunto de prueba y entrenamiento actual\n",
    "    X_test = X_folds[i]\n",
    "    Y_test = Y_folds[i]\n",
    "    \n",
    "    X_train = np.concatenate([X_folds[j] for j in range(k) if j != i])\n",
    "    Y_train = np.concatenate([Y_folds[j] for j in range(k) if j != i])\n",
    "    \n",
    "    #Creamos y entrenamos el modelo.\n",
    "\n",
    "    # Iniciar Regresion\n",
    "    clasificador = Regresion(700,0.00000001)\n",
    "    clasificador.train(X_train, Y_train)\n",
    "    \n",
    "    # Evalúa el modelo en el conjunto de prueba\n",
    "    y_pred_test = clasificador.predict(X_test)\n",
    "    porcentaje_aciertos=acuary(y_pred_test,Y_test)   \n",
    "    \n",
    "    # Almacena la precisión en la lista de accuracies\n",
    "    accuracies.append(porcentaje_aciertos)\n",
    "\n",
    "\n",
    "# Calcula la precisión promedio de precion en cada K-fold\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f'Precisión promedio: {average_accuracy:.2f}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo KNN\n",
    "\n",
    "#### Validacion por division del dataset en 70% entrenamiento 15% validación, 15% prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mezclamos los datos aleatoriamente\n",
    "indices = np.arange(len(x))\n",
    "np.random.shuffle(indices)\n",
    "X = x[indices]\n",
    "Y = y[indices]\n",
    "\n",
    "total_samples = len(X)\n",
    "train = int(0.7 * total_samples)\n",
    "validation = int(0.15 * total_samples)\n",
    "\n",
    "# Dividimoss los datos en conjuntos de entrenamiento, validación y prueba\n",
    "X_train = X[:train]\n",
    "y_train = Y[:train]\n",
    "\n",
    "X_val = X[train:train + validation]\n",
    "y_val = Y[train:train + validation]\n",
    "\n",
    "X_test = X[train + validation:]\n",
    "y_test = Y[train + validation:]\n",
    "\n",
    "#Entrenamos el modelo\n",
    "clasificador = KNN(k=5)\n",
    "clasificador.aprendizaje(X_train.T,y_train)\n",
    "\n",
    "# Evalúa el modelo en el conjunto de validacion\n",
    "y_val_pred=clasificador.clasificacion(X_val.T)\n",
    "aciertos_val=acuary(y_val_pred,y_val)\n",
    "\n",
    "print(f\"Precisión en el conjunto de validación: {aciertos_val:.2f}\")\n",
    "\n",
    "\n",
    "# Evalúa el modelo en el conjunto de prueba\n",
    "y_test_pred=clasificador.clasificacion(X_test.T)\n",
    "aciertos_prue=acuary(y_test_pred,y_test)\n",
    "\n",
    "print(f\"Precisión en el conjunto de prueba: {aciertos_prue:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validacion por K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el número de folds\n",
    "k = 5\n",
    "\n",
    "# Mezclamos los datos aleatoriamente\n",
    "indices = np.arange(len(x))\n",
    "np.random.shuffle(indices)\n",
    "X = x[indices]\n",
    "Y = y[indices]\n",
    "\n",
    "\n",
    "# Divide los datos en k grupos\n",
    "X_folds = np.array_split(X, k)\n",
    "Y_folds = np.array_split(Y, k)\n",
    "\n",
    "\n",
    "\n",
    "# Lista para almacenar los porcentajes de rendimiento\n",
    "accuracies=[]\n",
    "\n",
    "# Realizamos k iteraciones\n",
    "for i in range(k):\n",
    "    # Selecciona el conjunto de prueba y entrenamiento actual\n",
    "    X_test = X_folds[i]\n",
    "    Y_test = Y_folds[i]\n",
    "    \n",
    "    X_train = np.concatenate([X_folds[j] for j in range(k) if j != i])\n",
    "    Y_train = np.concatenate([Y_folds[j] for j in range(k) if j != i])\n",
    "    \n",
    "    #Creamos y entrenamos el modelo.\n",
    "\n",
    "    # Iniciar KNN\n",
    "    clasificador = KNN(k=5)\n",
    "    clasificador.aprendizaje(X_train.T,Y_train)\n",
    "    \n",
    "    # Evalúa el modelo en el conjunto de prueba\n",
    "    y_pred=clasificador.clasificacion(X_test.T)\n",
    "    porcentaje_aciertos=acuary(y_pred,Y_test)    \n",
    "    \n",
    "    # Almacena la precisión en la lista de accuracies\n",
    "    accuracies.append(porcentaje_aciertos)\n",
    "\n",
    "\n",
    "# Calcula la precisión promedio de precion en cada K-fold\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f'Precisión promedio: {average_accuracy:.2f}%')\n",
    "\n",
    "# Evaluamos la precision, Recall y  F1-Score\n",
    "classification_rep = classification_report(Y_test, y_pred)\n",
    "print(\"Informe de Clasificación:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo SVM\n",
    "#### Validacion por division del dataset en 70% entrenamiento 15% validación, 15% prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los tamaños de los conjuntos (70% entrenamiento, 15% validación, 15% prueba)\n",
    "total_samples = len(x)\n",
    "train = int(0.7 * total_samples)\n",
    "validation = int(0.15 * total_samples)\n",
    "\n",
    "# Dividimoss los datos en conjuntos de entrenamiento, validación y prueba\n",
    "X_train = x[:train]\n",
    "y_train = y[:train]\n",
    "\n",
    "X_val = x[train:train + validation]\n",
    "y_val = y[train:train + validation]\n",
    "\n",
    "X_test = x[train + validation:]\n",
    "y_test = y[train + validation:]\n",
    "\n",
    "# Crear y entrenar el modelo\n",
    "modelo = SVM(1,300,0.0000001)\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Validacion\n",
    "y_pred_val = modelo.prediccion(X_val)\n",
    "precision_val=acuary(y_val,y_pred_val)\n",
    "\n",
    "print(f\"Precisión en el conjunto de validación: {precision_val:.2f}\")\n",
    "\n",
    "# Predicción\n",
    "y_pred_test = modelo.prediccion(X_test)\n",
    "precision_test=acuary(y_test,y_pred_test)\n",
    "\n",
    "print(f\"Precisión en el conjunto de testeo: {precision_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validacion por K-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el número de folds\n",
    "k = 5\n",
    "\n",
    "# Mezclamos los datos aleatoriamente\n",
    "indices = np.arange(len(x))\n",
    "np.random.shuffle(indices)\n",
    "X = x[indices]\n",
    "Y = y[indices]\n",
    "\n",
    "\n",
    "# Divide los datos en k grupos\n",
    "X_folds = np.array_split(X, k)\n",
    "Y_folds = np.array_split(Y, k)\n",
    "\n",
    "\n",
    "\n",
    "# Lista para almacenar los porcentajes de rendimiento\n",
    "accuracies=[]\n",
    "\n",
    "# Realizamos k iteraciones\n",
    "for i in range(k):\n",
    "    # Selecciona el conjunto de prueba y entrenamiento actual\n",
    "    X_test = X_folds[i]\n",
    "    Y_test = Y_folds[i]\n",
    "    \n",
    "    X_train = np.concatenate([X_folds[j] for j in range(k) if j != i])\n",
    "    Y_train = np.concatenate([Y_folds[j] for j in range(k) if j != i])\n",
    "    \n",
    "    #Creamos y entrenamos el modelo.\n",
    "\n",
    "    # Iniciar Regresion\n",
    "    clasificador = SVM(1,300,0.0000001)\n",
    "    clasificador.fit(X_train, Y_train)\n",
    "    \n",
    "    # Evalúa el modelo en el conjunto de prueba\n",
    "    y_pred_test = clasificador.prediccion(X_test)\n",
    "    porcentaje_aciertos=acuary(y_pred_test,Y_test)   \n",
    "    \n",
    "    # Almacena la precisión en la lista de accuracies\n",
    "    accuracies.append(porcentaje_aciertos)\n",
    "\n",
    "\n",
    "# Calcula la precisión promedio de precion en cada K-fold\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f'Precisión promedio: {average_accuracy:.2f}%') \n",
    "\n",
    "\n",
    "# Evaluamos la precision, Recall y  F1-Score\n",
    "classification_rep = classification_report(Y_test, y_pred)\n",
    "print(\"Informe de Clasificación:\")\n",
    "print(classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Arboles de Desición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage.io import imread, imshow\n",
    "from pathlib import Path\n",
    "import pywt\n",
    "import pywt.data\n",
    "from PIL import Image\n",
    "from sklearn.decomposition import PCA # Para reducir la dimensionalidad\n",
    "from sklearn.preprocessing import StandardScaler # Para hacer la normalización\n",
    "\n",
    "def redimensionar(path):\n",
    "   imagenes=[]\n",
    "   entries = Path(path)\n",
    "   for entry in entries.iterdir():\n",
    "      imagenes.append(path + entry.name)\n",
    "\n",
    "   # Tamaño deseado para todas las imágenes\n",
    "   ancho_deseado = 300\n",
    "   alto_deseado = 200\n",
    "\n",
    "   # Carpeta de destino para las imágenes redimensionadas\n",
    "   carpeta_destino = \"imagenes_1\"\n",
    "\n",
    "   # Crea la carpeta de destino si no existe\n",
    "   if not os.path.exists(carpeta_destino):\n",
    "      os.makedirs(carpeta_destino)\n",
    "\n",
    "   # Recorre la lista de imágenes y redimensiona y guarda cada una\n",
    "   for ruta_imagen in imagenes:\n",
    "      imagen = Image.open(ruta_imagen)\n",
    "      \n",
    "      # Redimensiona la imagen al tamaño deseado con interpolación ANTIALIAS\n",
    "      imagen_redimensionada = imagen.resize((ancho_deseado, alto_deseado), Image.ADAPTIVE)\n",
    "      \n",
    "      # Obtiene el nombre de archivo sin la ruta\n",
    "      nombre_archivo = os.path.basename(ruta_imagen)\n",
    "      \n",
    "      # Define la ruta de archivo de destino en la carpeta de destino\n",
    "      ruta_destino = os.path.join(carpeta_destino, nombre_archivo)\n",
    "      \n",
    "      # Guarda la imagen redimensionada en la carpeta de destino\n",
    "      imagen_redimensionada.save(ruta_destino)   \n",
    "\n",
    "\n",
    "# Partición de la imagen en su esquina superior izquierda\n",
    "def Get_Feacture(picture, cortes):\n",
    "    LL = picture.copy()\n",
    "    for i in range(cortes): # Se corta la cantidad de veces determinada (2)\n",
    "        LL, (LH, HL, HH) = pywt.dwt2(LL, 'haar')\n",
    "    return LL.flatten().tolist() # Se convierte esta matriz representativa en un vector característico\n",
    "\n",
    "def Cargar_dataset_arbol():\n",
    "\n",
    "    path = './imagenes_1/'\n",
    "\n",
    "    vectores_caracteristicos = []\n",
    "\n",
    "    entries = Path(path)\n",
    "\n",
    "    # Iteramos la lista de directorios\n",
    "    for entry in entries.iterdir():\n",
    "        imagen = path + entry.name\n",
    "        picture = imread(imagen)\n",
    "        vector_caracteristico = Get_Feacture(picture, 2) # Se extrae el vector característico\n",
    "        vector_caracteristico.append(entry.name[0:3]) # Se agrega el label\n",
    "        vectores_caracteristicos.append(vector_caracteristico) # Agregamos ese nuevo vector característico\n",
    "\n",
    "    # Convertimos a numpy los datos de entrada\n",
    "    X = np.array(vectores_caracteristicos)[:, :-1]  # Elimina la última columna (labels) para dejar solo los datos de entrada\n",
    "\n",
    "    # Aplicamos PCA para reducir la dimensionalidad\n",
    "    pca = PCA(n_components=200) # Elegimos el número de componentes principales\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    # Normalizamos los datos\n",
    "    scaler = StandardScaler()\n",
    "    X_pca_normalized = scaler.fit_transform(X_pca)\n",
    "\n",
    "\n",
    "    # Obtenemos la lista de etiquetas\n",
    "    labels = np.array(vectores_caracteristicos)[:, -1]\n",
    "\n",
    "    return X_pca_normalized,labels\n",
    "\n",
    "\n",
    "# Redimensionamos las imagenes para trabajar todas con el mismo tamaño\n",
    "redimensionar(\"./images/\")\n",
    "\n",
    "# Definimos el dataset\n",
    "x,y= cargar_dataset()\n",
    "\n",
    "x=np.array(x)\n",
    "y=np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
